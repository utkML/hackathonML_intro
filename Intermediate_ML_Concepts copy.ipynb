{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate Machine Learning Concepts\n",
    "\n",
    "## Picking Up Speed in the Competition\n",
    "\n",
    "Aliens! Scientists have discovered life on another planet! The plant Zenon is crawling with alien creatures, and while we don't know much about these Zenon inhabitants yet, we have collected plenty of data from satellite images. Your job is to classify each creature into one of seven species that the researchers have distinguished so far.\n",
    "\n",
    "Here we build on our Introduction to Machine Learning talk, bolstering some of our methods as well as introducing some new approaches to the data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Save the Id's from the test set (we need them for the predictions)\n",
    "id_test = test['Id']\n",
    "X_test = test.drop('Id', axis=1)\n",
    "\n",
    "# remove constant columns\n",
    "remove = []\n",
    "for col in train.columns:\n",
    "    if train[col].std() == 0:\n",
    "        remove.append(col)\n",
    "        \n",
    "train.drop(remove, axis=1, inplace=True)\n",
    "test.drop(remove, axis=1, inplace=True)\n",
    "\n",
    "y_train = train['Alien_Type']\n",
    "X_train = train.drop(['Id', 'Alien_Type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New split for training and evaluation from training set.\n",
    "X_train, X_val, y_train, y_val= train_test_split(X_train, y_train, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Based Model\n",
    "\n",
    "In the Introductions to Machine Learning talk we used a Naive Bayes model. Here Let's give a shot at a tree based classifier, the Extra Trees Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass accuracy: 0.669080604534\n"
     ]
    }
   ],
   "source": [
    "#clf = ExtraTreesClassifier()\n",
    "#clf = DecisionTreeClassifier()\n",
    "#clf = GaussianNB()\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "# Predict the model on the validation set\n",
    "val_pred = clf.predict(X_val)\n",
    "\n",
    "# How did we do? Note: this competition uses Multiclass Accuracy!\n",
    "acc = accuracy_score(y_val, val_pred)\n",
    "print(\"Multiclass accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Analysis\n",
    "\n",
    "While ensemble tree models like the Extra Trees Classifier tend not to overfit, there is still that chance. One strategy to make our models more robust is to use a data preprocessing method called Principal Component Analysis (PCA). In essence, PCA is a data transformation that allows us to describe the variation in our data with fewer explanatory features. It is important to note that PCA does not throw away any of the features in our data, rather it re-expresses the information of our original features in lower dimensions. It is a method used to counteract what is known as the 'curse of dimensionality'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass accuracy: 0.52298488665\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=7)\n",
    "\n",
    "pca.fit(X_train)\n",
    "X_train_new = pca.transform(X_train)\n",
    "pca.fit(X_val)\n",
    "X_val_new = pca.transform(X_val)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train_new, y_train)\n",
    "# Predict the model on the validation set\n",
    "val_pred = clf.predict(X_val_new)\n",
    "\n",
    "# How did we do? Note: this competition uses Multiclass Accuracy!\n",
    "acc = accuracy_score(y_val, val_pred)\n",
    "print(\"Multiclass accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a multiclass accuracy score of $77.4$% on the validation set, and this is using information contained in only seven features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Fold Validation\n",
    "\n",
    "So far we have been trusting that the accuracy score we get on validation set will be indicative of what we get on the actual test set. But what if we just happened to get lucky on that random subset of the data? This is a legitimate concern. After all, a great one-time score could be a fluke... \n",
    "\n",
    "Luckily, there are ways that we can further safeguard our generalization estimates. One of these methods is to use cross fold validation. The idea here is pretty simple: don't just trust a single random split for your validation data, use many random splits into training and validation sets (folds), fit the model on the new training set and see what happens when we predict on the validation set. If you are consistently good across many random splits of the data, odds are you are going to do pretty well on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66921419095857404"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's recombine the train and validation sets\n",
    "X_train = pd.concat([X_train, X_val])\n",
    "y_train = pd.concat([y_train, y_val])\n",
    "\n",
    "cross_val_score(clf, X_train, y_train, cv=5, n_jobs=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca.fit(X_test)\n",
    "X_test_new = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We have our model, let's predict on the test set!\n",
    "y_pred = clf.predict(X_test_new)\n",
    "\n",
    "# Creating a submission\n",
    "# ------------------\n",
    "submission = pd.DataFrame({\"Id\":id_test, \"Alien_Type\":y_pred}).set_index(\"Id\")\n",
    "submission.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
